자연어 처리 개요

1. 단어표현 : one - hot 인코딩
2. Hashing trick을 이용한 word embedding
3. 단어표현 :  카운트 기반 방법 = Co-occurrence matrix



#### Hasing trick을 이용한 word embedding

> 단어장 만들 때 `apple` 단어 만들기
>
> 1. 견출지 (a,b,c,...,z) 까지 "Indexing"
>
>    -장점 : 나중에 찾을 때 빠르다 -> 검색속도 빠름
>
>    -단점 : 견출지 붙일 때 몇장마다 붙일지 미리 정해야함, a=4, b=5장 등등 
>
>    ​			-> memory 비효율성 (key의 범위가 넓다면 더욱 비효율성 높아진다.)
>
> 2. "apple"을 수치변환을 해보자
>
>    -장점 : 메모리 효울성, 특히 넓은 key 영역에서 효율적
>
>    -단점 : apple ---> 584 / 100 = 84
>
>    ​									 /    80 = 36 이라면,  80page - 36th line
>
>    ​			apple -> 584, tiger -> 584 가 우연히 같을 수 있다. 
>
>    ​				=> `Collision`발생 -> overflow 처리

* `Hasing`을 단어 표현에 적용해보자
* oov의 보완점으로 나온 방법 중 하나
* vocabulary의 크기를 미리 지정하고, 단어들을 hash table에 대응시키는 방식이다.
* vocabulary가 아예 없기 때문에 oov 문제점이 없다. 단, collision의 문제가 발생한다.



#### 단어표현: 카운트 기반 방법 => Co-occurrence matrix (동시 출현 행렬)

* 학습기반이 아니라, 빈도기반이다

* 단어들 간의 관계 정보가 내포되어 있다.

* Co-occurrence matrix를 사용하여 단어를 수치화 한다. 

  ​	(one hot처럼 vocabulary의 갯수 -> emb.size로 줄여서 emb.vector를 만든다.)

* 대칭행렬이다.

* 단어간 유사도가 존재할 수 있다.(직교가 아닌 단어쌍이 존재한다 -> 유사도로써 의미가 있을까? yes!)

* 큰 corpus에 대해서 단어가 많으므로 동시 출현 횟수가 적어 '0'이 많이 등장한다. 희소행렬(sparse matrix)

* 특이값 분해(SVD) 등을 사용해 단어 벡터의 차원을 줄일 수 있다.

* 주변 단어에 한정한 `Skip-Gram` 과 다르게 `Co-occurrence matrix`는 전체 단어를 고려했다.

* 확률로 표시한 "동시 발생 확률".  단, size가 클수록 커진다.



> ###### CountVectorizer ([API Reference](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer))
>
> : 문서목록에서 각 문서의 feature(문장의 특징) 노출수를 가중치로 설정한 BOW 벡터를 만든다.****
>
> ###### TfidfVectorizer ([API Reference](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer))
>
> : 문서목록에서 각 문서의 feature를 tf-idf 값을 가중치로 설정한 BOW 벡터를 만든다. ([TF-IDF에 대한 참고](https://thinkwarelab.wordpress.com/2016/11/14/ir-tf-idf-에-대해-알아봅시다))****
>
> ###### HashingVectorizer ([API Reference](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html))
>
> : CountVectorizer, TfidfVectorizer 와 달리 벡터화 할때 모든 feature 에 대해 사전을 만들지 않고, 해싱함수를 통해 벡터안의 인덱스를 특정하도록 한다. 큰 사전을 만들 필요가 없어 메모리 소모가 적어 대용량 텍스트를 벡터화 할때 많이 쓰인다. ([Hashing Trick에 대한 참고](http://i5on9i.blogspot.kr/2016/06/machine-learning-feature-extraction.html))
>

```python

from sklearn.feature_extraction.text import CountVectorizer

docs = ['성진과 창욱은 야구장에 갔다',
        '성진과 태균은 도서관에 갔다',
        '성진과 창욱은 공부를 좋아한다']
count_model = CountVectorizer(ngram_range=(1,1)) #unigram
	#count_model = CountVectorizer(ngram_range=(1,2))  #bigram
x = count_model.fit_transform(docs)

# 문서에 사용된 사전을 조회한다.
count_model.vocabulary_
	# => {'성진과': 3, '창욱은': 6, '야구장에': 4, '갔다': 0, '태균은': 7, '도서관에': 2, '공부를': 1, '좋아한다': 5}
     
x.toarray()
	#=> [[1 0 0 1 1 0 1 0]
 		[1 0 1 1 0 0 0 1]
 		[0 1 0 1 0 1 1 0]]
       
x.T.toarray()
	#=> [[1 1 0]
		 [0 0 1]
		 [0 1 0]
		 [1 1 1]
		 [1 0 0]
 		 [0 0 1]
 		 [1 0 1]
 		 [0 1 0]]
        
xc = x.T * x # this is co-occurrence matrix in sparse csr format
xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0
print(xc.toarray())
	#=> [[0 0 1 2 1 0 1 1]
		 [0 0 0 1 0 1 1 0]
 	 	 [1 0 0 1 0 0 0 1]
		 [2 1 1 0 1 1 2 1]
		 [1 0 0 1 0 0 1 0]
		 [0 1 0 1 0 0 1 0]
		 [1 1 0 2 1 1 0 0]
		 [1 0 1 1 0 0 0 0]]
#             0       1       2       3      4        5      6      7
#             갔다   공부를  도서관에  성진과  야구장에  좋아한다  창욱은  태균은
#0 갔다         0      0      1       2       1       0       1      1
#1 공부를		 0      0      0       1        0       1       1      0
#2 도서관에		1       0      0       1       0       0       0      1
#3 성진과		 2      1      1        0       1       1       2      1
#4 야구장에		1      0       0       1        0      0        1      0
#5 좋아한다		0      1       0       1        0      0        1      0
#6 창욱은		 1      1       0       2       1       1        0      0
#7 태균은		 1      0       1       1       0       0        0      0
```

#### Tfidf를 이용한 유사도 측정

* 텍스트 유사도에서 사용되는 것 (자카드 유사도, 코사인 유사도, 유클리디안 유사도, 맨하탄 유사도  )

```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

sent = ("휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.", 
        "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.") 

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(sent).toarray() # shape=(2,17)
np.round(tfidf_matrix, 3)
	#=>  [[0.000 0.324 0.000 0.000 0.324 0.324 0.324 0.324 0.324 0.324 0.000 0.231
  		    0.324 0.231 0.000 0.000 0.231]
		  [0.365 0.000 0.365 0.365 0.000 0.000 0.000 0.000 0.000 0.000 0.365 0.259
  			0.000 0.259 0.365 0.365 0.259]]
tfidf_vectorizer.vocabulary_
	#=> {'휴일': 16, '오늘': 7, '서쪽': 5, '중심': 12, '으로': 8, '폭염': 13, '이어졌는데요': 9, 			'내일': 1, '반가운': 4, '소식': 6, '있습니다': 11, '피해서': 14, '놀러왔다가': 2, '갑작스			 런': 0, '인해': 10, '망연자실': 3, '하고': 15}

```



##### 1. 자카드 유사도

 * 두 문장을 각 각 단어의 집합으로 만든 뒤, 두 집합을 통해 유사도를 측정(두 집합의 교집합 / 두 집합의 합집합)

```python
sent_1 = set(sent[0].split())
sent_2 = set(sent[1].split())

# 합집합과 교집합을 구한다.
hap_set = sent_1 | sent_2
gyo_set = sent_1 & sent_2
print(hap_set, '\n')
	#=> {'이어졌는데요,', '소식', '도', '휴일', '을', '에', '하고', '이', '망연자실', '폭염', 			 '로','있습니다.', '내일', '피해서', '중심', '비', '인해', '은', '오늘', '반가운', '놀러왔		    다가','으로', '인', '갑작스런', '서쪽'}
print(gyo_set, '\n')
 	#=> {'비', '휴일', '을', '폭염', '있습니다.'} 

jaccard = len(gyo_set) / len(hap_set)  # 5/25
print(jaccard)  # 0.2
```

##### 2. 코사인 유사도

​		두 개의 벡터값에서 코사인 각도를 구하기

* -1 - 1 사이의 값 / 1에 가까울 수록 => 유사하다 , 유사하지 않으면 직교
* 두 벡터 간의 각도를 구하기 때문에 방향 성의 개념이 더해진다.

```python
from sklearn.metrics.pairwise import cosine_similarity

d = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
print(d) #[[0.17952266]]

```

##### 3. 유클리디안 유사도

* 유클리디안 거리를 이용/ n차원에서 두 점 사이의 최단 거리를 구하는 접근 방식
* 범위가 정해져 있지 않기에, 딥러닝 할 때는 L1 정규화(L1 normalize)를 사용한다.

```python
from sklearn.metrics.pairwise import euclidean_distances

d = euclidean_distances(tfidf_matrix[0:1], tfidf_matrix[1:2])
print(d)  	#array([[1.28099753]])

#정규화
def l1_normalize(v):
    return v / np.sum(v)

tfidf_norm_l1 = l1_normalize(tfidf_matrix)
d = euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])
print(d)  	#[[0.20491229]]
```

##### 4. 맨하탄 유사도

* 맨하탄 거리를 이용/ L1(L1 Distance)라고 불리운다.

```python
from sklearn.metrics.pairwise import manhattan_distances

d = manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])
print(d) 	# [[0.77865927]]

```

#### Hashing Vectorizer를 이용한 유사도 측정

*  자카드 유사도, 코사인 유사도, 유클리디안 유사도, 맨하탄 유사도 



#### 데이터 이해하기: 탐색적 데이터 분석

* 데이터를 제대로 분석하기 전에 데이터 자체에 대한 이해를 하자! ex) 데이터의 구조, 통계적 특성 등
























